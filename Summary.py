from hazm import Normalizer, SentenceTokenizer, WordTokenizer
from langdetect import detect
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import heapq

def summarize_farsi(text):
    normalizer = Normalizer()
    text = normalizer.normalize(text)

    sent_tokenizer = SentenceTokenizer()
    word_tokenizer = WordTokenizer()

    sentences = sent_tokenizer.tokenize(text)
    word_freq = {}

    for sent in sentences:
        words = word_tokenizer.tokenize(sent)
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1

    sentence_scores = {}
    for sent in sentences:
        for word in word_tokenizer.tokenize(sent):
            if word in word_freq:
                sentence_scores[sent] = sentence_scores.get(sent, 0) + word_freq[word]

    summary_sentences = heapq.nlargest(2, sentence_scores, key=sentence_scores.get)
    return " ".join(summary_sentences)


def summarize_english(text):
    parser = PlaintextParser.from_string(text, SumyTokenizer("english"))
    summarizer = TextRankSummarizer()
    summary = summarizer(parser.document, 2)  # 2 Ø¬Ù…Ù„Ù‡ Ù…Ù‡Ù…

    return " ".join([str(sentence) for sentence in summary])


# --- Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ ---
text = input("ğŸ“ Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:\n")

try:
    lang = detect(text)

    if lang == 'fa':
        print("\nğŸ“Œ Ø²Ø¨Ø§Ù†: ÙØ§Ø±Ø³ÛŒ")
        print("âœ… Ø®Ù„Ø§ØµÙ‡ Ù…ØªÙ†:")
        print(summarize_farsi(text))

    elif lang == 'en':
        print("\nğŸ“Œ Language: English")
        print("âœ… Summary:")
        print(summarize_english(text))

    else:
        print("â›” Ø²Ø¨Ø§Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ù„Ø·ÙØ§Ù‹ ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")

except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {e}")
